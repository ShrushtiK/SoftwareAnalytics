{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a51e73",
   "metadata": {},
   "source": [
    "## Pull request level analysis\n",
    "One part of analysis carried out for both our research questions considers each pull request as a seperate data point, rather than the whole repository. Here, for each pull request, we are calculating the time-to-merge metric and listing the timezones of participants in that pull request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cabbab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from timezonefinder import TimezoneFinder\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tf = TimezoneFinder()\n",
    "\n",
    "def query_timezone(location):\n",
    "    url = 'https://nominatim.openstreetmap.org/search'\n",
    "    params = {'q': location, 'format': 'json' }\n",
    "    headers = {\n",
    "        'User-Agent': 's.u.kaul@student.rug.nl'\n",
    "    }\n",
    "    response = requests.get(url, params = params, headers = headers)\n",
    "    response_data = response.json()\n",
    "\n",
    "    if response_data:\n",
    "        lat, lon = response_data[0]['lat'], response_data[0]['lon']\n",
    "        if lat is not None and lon is not None:\n",
    "            return tf.timezone_at(lat=float(lat), lng=float(lon))       \n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_timezone(text):\n",
    "    label_text = nlp(text)\n",
    "    gpe_entities = [ent.text for ent in label_text.ents if ent.label_ == 'GPE']\n",
    "    return query_timezone(' '.join(gpe_entities)) if gpe_entities else None\n",
    "\n",
    "def time_difference(d1, d2, unit='hours'):\n",
    "    d1 = datetime.strptime(d1, '%Y-%m-%dT%H:%M:%SZ')\n",
    "    d2 = datetime.strptime(d2, '%Y-%m-%dT%H:%M:%SZ')\n",
    "    \n",
    "    time_diff = d2 - d1\n",
    "    \n",
    "    if unit == 'hours':\n",
    "        return time_diff.total_seconds() / 3600\n",
    "    elif unit == 'minutes':\n",
    "        return time_diff.total_seconds() / 60\n",
    "    else:\n",
    "        raise ValueError(\"Invalid unit. Please use 'hours' or 'minutes'.\")\n",
    "        \n",
    "_TOKEN = \"<your-token>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2207e71",
   "metadata": {},
   "source": [
    "We are exploiting the pull requests and collaborator's dataset already extracted by the data collection section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e70a303",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/4_pull_requests.json\", \"r\") as f1:\n",
    "    pr_data = json.load(f1)\n",
    "with open(\"data/4_collaborators_and_users_with_timezone.json\") as f2:\n",
    "    user_location_data = json.load(f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0402c529",
   "metadata": {},
   "source": [
    "In this case, each pull request from each repository is analysed to derive the people that participated in the pull request (and thereby the timezones they are participating from) i.e. the author of the pull request, reviewers of the pull request and people who commented on the pull request. For a participant that doesn't happen to be a collaborator in the repository which the pull request in question is from, we won't explicitly have their location information and try to extract the timezone they are from by querying for their location as well (Note: this code logic has also been replicated from the data collection notebook and the implementation is discussed in Section 5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1291a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def check_rate_limit(headers):\n",
    "    rate_limit_response = requests.get('https://api.github.com/rate_limit', headers=headers)\n",
    "    rate_limit_info = rate_limit_response.json()\n",
    "    core_rate_limit = rate_limit_info['resources']['core']\n",
    "    print(\"Rate Limit Info:\")\n",
    "    print(f\"Limit: {core_rate_limit['limit']}\")\n",
    "    print(f\"Used: {core_rate_limit['used']}\")\n",
    "    print(f\"Remaining: {core_rate_limit['remaining']}\")\n",
    "    print(f\"Reset: {core_rate_limit['reset']}\")\n",
    "\n",
    "rq3_data = []\n",
    "\n",
    "for repo in pr_data:\n",
    "    considered_prs = []\n",
    "    author_submitter_location_data = []\n",
    "    for pr in repo[\"pull_requests\"]:\n",
    "        if pr[\"created_at\"] and pr[\"merged_at\"] is not None:\n",
    "            if \"[bot]\" not in pr[\"user\"][\"login\"]:\n",
    "                considered_prs.append(pr)\n",
    "                \n",
    "    for pr in considered_prs:\n",
    "        print(f\"Starting for repo {repo['full_name']}\")\n",
    "        repo_user_data = []\n",
    "        for u in user_location_data:\n",
    "            if u[\"name\"] == repo[\"full_name\"]:\n",
    "                repo_user_data = u[\"users_location\"]\n",
    "                break\n",
    "        \n",
    "        repo_users = [item[\"login\"] for item in repo_user_data if \"login\" in item]\n",
    "        \n",
    "        author = pr[\"user\"][\"login\"]\n",
    "\n",
    "        reviewers = []\n",
    "        \n",
    "        for reviewer in pr[\"requested_reviewers\"]:\n",
    "            reviewers.append(reviewer[\"login\"])        \n",
    "        #reviewers = pr[\"requested_reviewers\"] if pr[\"requested_reviewers\"] != [] else [] \n",
    "        \n",
    "        commentators = []\n",
    "        \n",
    "        url = pr[\"comments_url\"]\n",
    "        headers = {\n",
    "        'Authorization': f'token {_TOKEN}',\n",
    "        'Accept': 'application/vnd.github.v3+json'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code == 403:\n",
    "            print(\"Rate limited. Waiting 5 minutes...\")\n",
    "            check_rate_limit(headers=headers)\n",
    "            time.sleep(300)\n",
    "\n",
    "        elif response.status_code != 200:\n",
    "            # print error and try to fetch results of the current page again\n",
    "            print(f'Error code {response.status_code}: {response.reason} for respository')  \n",
    "\n",
    "        else: # if no error, process results and continue to next page\n",
    "            comments = response.json()\n",
    "            #print(data)\n",
    "            #print(data[\"closed_by\"])\n",
    "            \n",
    "        if comments != []:\n",
    "            for comment in comments:\n",
    "                commentators.append(comment[\"user\"][\"login\"])\n",
    "        #print(\"sleeping for 1 seconds\")\n",
    "        #time.sleep(1)\n",
    "\n",
    "        print(f\"authors {[author]} reviewers {reviewers} commentators {commentators}\")        \n",
    "        pr_participants = list(set([author] + reviewers + commentators))\n",
    "        \n",
    "        \n",
    "        pr_timezones = []\n",
    "        \n",
    "        if len(pr_participants) < 2: \n",
    "            print(\"Not enough participants\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Enough participants {len(pr_participants)}\")\n",
    "            \n",
    "        for participant in pr_participants:\n",
    "            if participant in repo_users:\n",
    "                for loc in repo_user_data:\n",
    "                    if loc[\"login\"] == participant:\n",
    "                            location = None\n",
    "                            if loc[\"location\"] is not None:\n",
    "                                location = loc[\"location\"]\n",
    "                            elif loc[\"bio\"] is not None:\n",
    "                                location = loc[\"bio\"]\n",
    "                            elif loc[\"company\"] is not None:\n",
    "                                location = loc[\"company\"]\n",
    "                            if location is not None:\n",
    "                                pr_timezones.append(location)                            \n",
    "                        \n",
    "            else:\n",
    "                url = f'https://api.github.com/users/{participant}'\n",
    "                headers = {\n",
    "                'Authorization': f'token {_TOKEN}',\n",
    "                'Accept': 'application/vnd.github.v3+json'\n",
    "                }\n",
    "                response = requests.get(url, headers=headers)\n",
    "\n",
    "                if response.status_code == 403:\n",
    "                    print(\"Rate limited. Waiting 5 minutes...\")\n",
    "                    check_rate_limit(headers=headers)\n",
    "                    time.sleep(300)\n",
    "\n",
    "                elif response.status_code != 200:\n",
    "                    # print error and try to fetch results of the current page again\n",
    "                    print(f'Error code {response.status_code}: {response.reason} for respository ')  \n",
    "\n",
    "                else: # if no error, process results and continue to next page\n",
    "                    user = response.json()\n",
    "                    \n",
    "                #user_timezone = {}\n",
    "                #user_timezone['login'] = user[\"login\"]\n",
    "                #location = None\n",
    "                #if user[\"location\"] is not None:\n",
    "                location_l = get_timezone(user[\"location\"]) if user[\"location\"] is not None else None\n",
    "                if location_l is None:\n",
    "                    location_b = get_timezone(user[\"bio\"]) if user[\"bio\"] is not None else None\n",
    "                    if location_b is None:\n",
    "                        location_c = get_timezone(user[\"company\"]) if user[\"company\"] is not None else None\n",
    "                        if location_c is not None:\n",
    "                            pr_timezones.append(location_c)\n",
    "                    else:\n",
    "                        pr_timezones.append(location_b)\n",
    "                else:\n",
    "                    pr_timezones.append(location_l)\n",
    "        \n",
    "        pr_timezones_final = list(set(pr_timezones))\n",
    "        \n",
    "        pr_timezones_map = {}\n",
    "        \n",
    "        for timezone in pr_timezones:\n",
    "            if timezone in pr_timezones_map:\n",
    "                pr_timezones_map[timezone] += 1\n",
    "            else:\n",
    "                pr_timezones_map[timezone] = 1\n",
    "        \n",
    "        if pr_timezones_final:\n",
    "            rq3_data.append({\n",
    "                \"repository\": repo[\"full_name\"],\n",
    "                \"participants_timezones\": pr_timezones_map,\n",
    "                \"time_to_merge\": time_difference(pr['created_at'], pr['merged_at'],'hours')\n",
    "            })\n",
    "            \n",
    "            with open(\"data/pr_participation.json\", \"w+\") as f:\n",
    "                json.dump(rq3_data, f)\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3252ff78",
   "metadata": {},
   "source": [
    "From this list of participants and the time to merge value for each PR, we are weeding out PR where we could only find the location of one participant as for collaboration, we need at least two participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d7fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/pr_participation.json\", \"r\") as f1:\n",
    "    pr_timezone_merge_data = json.load(f1)\n",
    "pr_timezone_merge_data_filtered = []\n",
    "for pr in pr_timezone_merge_data:\n",
    "    if len(pr[\"participants_timezones\"]) == 1 and list(pr[\"participants_timezones\"].values())[0] == 1:\n",
    "        continue\n",
    "    else:\n",
    "        pr_timezone_merge_data_filtered.append(pr)\n",
    "\n",
    "with open(\"data/pr_participation_filtered.json\", \"w\") as f:\n",
    "    json.dump(pr_timezone_merge_data_filtered, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
