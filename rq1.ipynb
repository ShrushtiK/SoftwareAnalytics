{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research Question 1\n",
    "Distinct timezones for each repository and their correlation with time-to-merge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each repositoy, we retrieved the location of all collaborators. In a json file, we placed all collaborators that have their location on their GitHub profile, removing all collaborators that have 'Null' values as their locations. Furthermore, we computed the average time-to-merge for all pull requests in the respective repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def time_difference(d1, d2, unit='hours'):\n",
    "    d1 = datetime.strptime(d1, '%Y-%m-%dT%H:%M:%SZ')\n",
    "    d2 = datetime.strptime(d2, '%Y-%m-%dT%H:%M:%SZ')\n",
    "    \n",
    "    time_diff = d2 - d1\n",
    "    \n",
    "    if unit == 'hours':\n",
    "        return time_diff.total_seconds() / 3600\n",
    "    elif unit == 'minutes':\n",
    "        return time_diff.total_seconds() / 60\n",
    "    else:\n",
    "        raise ValueError(\"Invalid unit. Please use 'hours' or 'minutes'.\")\n",
    "\n",
    "f1 = open(\"data/4_collaborators_and_users_with_timezone.json\")\n",
    "f2 = open(\"data/4_pull_requests.json\")\n",
    "data_tz = json.load(f1)\n",
    "data_pr = json.load(f2)\n",
    "\n",
    "repos = []\n",
    "\n",
    "for item in data_pr:\n",
    "    #print(f\"repo name of pr is {item['full_name']}\")\n",
    "    pull_requests = []\n",
    "\n",
    "    for entry in item['pull_requests']:\n",
    "        if (entry['created_at'] and entry['merged_at']) is not None:\n",
    "            pull_requests.append(time_difference(entry['created_at'], entry['merged_at'],'hours'))\n",
    "    if len(pull_requests) == 0:\n",
    "        average_pull_requests = 0\n",
    "    elif len(pull_requests) > 0:\n",
    "        average_pull_requests = sum(pull_requests) / len(pull_requests)\n",
    "    \n",
    "    repos.append({\n",
    "        \"repo_name\": item['full_name'],\n",
    "        \"pull_requests\": pull_requests,\n",
    "        \"pull_request_average\": average_pull_requests\n",
    "    })\n",
    "\n",
    "\n",
    "final_repos = []\n",
    "i = 0\n",
    "for item in data_tz:\n",
    "    #print(f\"repo name is {item['name']}\")\n",
    "    locations = []\n",
    "    for entry in item['users_location']:\n",
    "        if 'bot' not in entry['login']:\n",
    "            if entry['location'] is not None:\n",
    "                locations.append(entry['location'])\n",
    "            elif entry['bio'] is not None:\n",
    "                locations.append(entry['bio'])\n",
    "                #print(\"biooooooooooooooooooooooooooooooooooo\")\n",
    "            elif entry['company'] is not None:\n",
    "                #print(\"companyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\")\n",
    "                locations.append(entry['company'])\n",
    "\n",
    "    if len(locations) != 0:\n",
    "        repo_index = next((i for i, r in enumerate(repos) if r[\"repo_name\"] == item['name']), None)\n",
    "        if repo_index is not None:\n",
    "            if repos[repo_index]['pull_request_average'] != 0:\n",
    "                final_repos.append({\n",
    "                    \"repo_name\": item['name'],\n",
    "                    \"locations\": locations,\n",
    "                    \"time_to_merge\": repos[repo_index]['pull_requests'],\n",
    "                    \"time_to_merge_average\": repos[repo_index]['pull_request_average']\n",
    "                })\n",
    "        i += 1\n",
    "\n",
    "with open(\"RQ1_2_Locations_PR.json\", \"w\") as outfile:\n",
    "    json.dump(final_repos, outfile)\n",
    "print(\"done writing to json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data gathered before, we process the data so that we place each location in it's respective UTC timezone. We count how many collaborators of a repository belong to a distinct timezone. We also keep the average time-to-merge for each repositoy. After we place all this data into a json file, we will perform statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "from datetime import datetime, timezone\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "f = open(\"RQ1_2_Locations_PR.json\")\n",
    "location_data = json.load(f)\n",
    "\n",
    "# Initialize the output dictionary\n",
    "output_data = {}\n",
    "\n",
    "for item in location_data:\n",
    "    repo_name = item['repo_name']\n",
    "    if repo_name not in output_data:\n",
    "        output_data[repo_name] = {\n",
    "            \"average_pr_time\": item['time_to_merge_average']\n",
    "        }\n",
    "\n",
    "    timezone_counts = defaultdict(int)\n",
    "    timezone_offsets = []\n",
    "    for location in item['locations']:\n",
    "        try:\n",
    "            tz_obj = pytz.timezone(location)\n",
    "            utc_offset = tz_obj.utcoffset(datetime.now())\n",
    "            if utc_offset.total_seconds() == 0:\n",
    "                tz_offset_str = \"UTC+0\"\n",
    "            else:\n",
    "                tz_offset_str = f\"UTC{'+' if utc_offset.total_seconds() > 0 else '-'}{abs(int(utc_offset.total_seconds() / 3600))}\"\n",
    "            timezone_counts[tz_offset_str] += 1\n",
    "            \n",
    "            # Extract the numeric offset from the string\n",
    "            if tz_offset_str.startswith(\"UTC+\"):\n",
    "                timezone_offsets.append(int(tz_offset_str[4:]))\n",
    "            elif tz_offset_str.startswith(\"UTC-\"):\n",
    "                timezone_offsets.append(-int(tz_offset_str[4:]))\n",
    "            else:\n",
    "                timezone_offsets.append(0)\n",
    "        except pytz.exceptions.UnknownTimeZoneError:\n",
    "            print(f\"Unknown timezone: {location}\")\n",
    "\n",
    "    for timezone, count in timezone_counts.items():\n",
    "        output_data[repo_name][timezone] = count\n",
    "    output_data[repo_name]['nr_timezones'] = len(timezone_counts)\n",
    "\n",
    "    # Calculate the timezone difference\n",
    "    min_offset = min(timezone_offsets)\n",
    "    max_offset = max(timezone_offsets)\n",
    "    timezone_difference = abs(max_offset - min_offset)\n",
    "    output_data[repo_name]['timezone_difference'] = timezone_difference\n",
    "\n",
    "with open(\"RQ1_utc_count_avg_prs.json\", \"w\") as outfile:\n",
    "    json.dump(output_data, outfile)\n",
    "\n",
    "print(\"done writing to json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do:\n",
    "1) histogram with ordered timezones for all repositories.\n",
    "2) statistical analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
